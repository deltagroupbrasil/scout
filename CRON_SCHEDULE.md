# üïê Cronograma de Scraping Automatizado

## üìÖ Estrat√©gia de Execu√ß√£o

O sistema executa scraping automatizado **6 vezes por dia**, de 10 em 10 minutos, entre **5h e 6h da manh√£** (hor√°rio de Bras√≠lia).

### ‚è∞ Hor√°rios de Execu√ß√£o

| Execu√ß√£o | Hor√°rio Brasil | Hor√°rio UTC | Empresas/Execu√ß√£o |
|----------|----------------|-------------|-------------------|
| 1        | 05:00          | 08:00       | 20                |
| 2        | 05:10          | 08:10       | 20                |
| 3        | 05:20          | 08:20       | 20                |
| 4        | 05:30          | 08:30       | 20                |
| 5        | 05:40          | 08:40       | 20                |
| 6        | 05:50          | 08:50       | 20                |

**Total por dia:** At√© **120 empresas √∫nicas**

## üéØ Vantagens desta Estrat√©gia

### 1. **Distribui√ß√£o de Carga**
- Execu√ß√µes menores (20 empresas) evitam timeouts
- Timeout de 5 minutos (Vercel Fluid Compute) √© suficiente

### 2. **Maior Volume**
- 120 empresas/dia (vs 50 no modelo anterior)
- Aumenta em 140% a capacidade de prospec√ß√£o

### 3. **Resili√™ncia**
- Se 1 execu√ß√£o falhar, as outras 5 continuam
- No modelo anterior (1x/dia), uma falha = zero leads

### 4. **Hor√°rio Estrat√©gico**
- 5h-6h da manh√£ = per√≠odo de baixo tr√°fego
- Leads ficam dispon√≠veis desde cedo para o time

## üìä C√°lculo de Leads Esperados

Assumindo uma m√©dia de **1.5 vagas por empresa**:
- 120 empresas √ó 1.5 vagas = **~180 leads/dia**
- **~1,260 leads/semana**
- **~5,400 leads/m√™s**

## üîß Configura√ß√£o T√©cnica

### Arquivo: `vercel.json`
```json
{
  "crons": [
    { "path": "/api/cron/scrape-leads", "schedule": "0 8 * * *" },
    { "path": "/api/cron/scrape-leads", "schedule": "10 8 * * *" },
    { "path": "/api/cron/scrape-leads", "schedule": "20 8 * * *" },
    { "path": "/api/cron/scrape-leads", "schedule": "30 8 * * *" },
    { "path": "/api/cron/scrape-leads", "schedule": "40 8 * * *" },
    { "path": "/api/cron/scrape-leads", "schedule": "50 8 * * *" }
  ]
}
```

### Endpoint: `/api/cron/scrape-leads`
- **Timeout:** 300s (5 minutos)
- **Empresas por execu√ß√£o:** 20
- **Query:** "Controller OR CFO OR Gerente Financeiro OR Diretor Financeiro OR Controladoria S√£o Paulo"

## üìà Monitoramento

Para verificar o hist√≥rico de execu√ß√µes:

```bash
# Verificar logs de scraping
DATABASE_URL="..." npx tsx scripts/check-scrape-logs.ts

# Verificar leads criados por data
DATABASE_URL="..." npx tsx scripts/check-leads-by-date.ts
```

### M√©tricas Esperadas

**Sucesso:**
- 6 execu√ß√µes/dia com status "success"
- ~20 leads criados por execu√ß√£o
- Dura√ß√£o: 60-180s por execu√ß√£o

**Alertas:**
- Taxa de sucesso < 80% (< 5 execu√ß√µes com sucesso)
- Dura√ß√£o > 240s (pr√≥ximo do timeout)
- Leads criados < 10 por execu√ß√£o

## üö® Troubleshooting

### Cron n√£o est√° executando?
1. Verificar Vercel Dashboard ‚Üí Cron Jobs
2. Verificar `CRON_SECRET` nas vari√°veis de ambiente
3. Testar manualmente: `curl https://leapscout.vercel.app/api/cron/scrape-leads -H "Authorization: Bearer YOUR_SECRET"`

### Execu√ß√µes falhando?
1. Verificar logs no Vercel Dashboard
2. Verificar se Bright Data tem cr√©ditos
3. Verificar se DATABASE_URL est√° configurado
4. Verificar timeout (max 300s no Hobby plan)

### Muitas duplicatas?
- O sistema j√° valida duplicatas por `jobUrl`
- Se necess√°rio, ajustar limite de empresas para 15 ou 10

## üîÑ Altera√ß√µes Futuras

### Se precisar aumentar volume:
- Adicionar mais hor√°rios (ex: 6h-7h tamb√©m)
- Aumentar para 25-30 empresas/execu√ß√£o

### Se precisar economizar cr√©ditos:
- Reduzir para 4 execu√ß√µes (a cada 15min)
- Reduzir para 15 empresas/execu√ß√£o
