# üì° LeapScout - API Documentation

## üîê Autentica√ß√£o

Todas as APIs (exceto login) requerem autentica√ß√£o via NextAuth session.

---

## üìã Endpoints

### 1. **GET /api/leads**

Lista todos os leads com filtros opcionais.

**Query Parameters:**
```
?status=NEW|CONTACTED|QUALIFIED|DISCARDED|ALL
&search=nome_da_empresa
&dateRange=7d|30d|all
&page=1
&pageSize=20
```

**Response:**
```json
{
  "data": [...],
  "total": 100,
  "page": 1,
  "pageSize": 20,
  "totalPages": 5
}
```

---

### 2. **GET /api/leads/[id]**

Retorna detalhes completos de um lead espec√≠fico.

**Response:**
```json
{
  "id": "uuid",
  "company": {...},
  "jobTitle": "Controller S√™nior",
  "suggestedContacts": [...],
  "triggers": [...],
  "notes": [...]
}
```

---

### 3. **PATCH /api/leads/[id]**

Atualiza status ou flag isNew de um lead.

**Body:**
```json
{
  "status": "CONTACTED",
  "isNew": false
}
```

---

### 4. **POST /api/notes**

Cria uma nova nota em um lead.

**Body:**
```json
{
  "leadId": "uuid",
  "content": "Liga√ß√£o agendada para amanh√£"
}
```

---

### 5. **POST /api/scrape** (Manual)

Executa scraping manual de leads.

**Body:**
```json
{
  "query": "Controller OR CFO S√£o Paulo"
}
```

---

### 6. **GET /api/cron/scrape-leads** (Autom√°tico)

Endpoint para cron job autom√°tico (executado diariamente √†s 6h).

**Headers:**
```
Authorization: Bearer {CRON_SECRET}
```

**Schedule (Vercel Cron):**
```json
{
  "crons": [{
    "path": "/api/cron/scrape-leads",
    "schedule": "0 6 * * *"
  }]
}
```

---

## üîß Configura√ß√£o das APIs Externas

### 1. **Claude API (Anthropic)**

Para gerar insights com IA:

1. Crie uma conta em https://console.anthropic.com
2. Gere uma API key
3. Adicione ao `.env`:
```env
CLAUDE_API_KEY="sk-ant-api03-..."
```

**Custo estimado:** ~R$ 100/m√™s (dependendo do volume)

---

### 2. **Bright Data API (LinkedIn Scraping)**

Para scraping de vagas no LinkedIn:

1. Crie uma conta em https://brightdata.com
2. Configure Web Scraper API
3. Adicione ao `.env`:
```env
BRIGHT_DATA_API_KEY="your-api-key"
```

**Custo estimado:** ~R$ 200/m√™s

**Nota:** Implementa√ß√£o completa requer configura√ß√£o adicional no `linkedin-scraper.ts`

---

### 3. **Hunter.io (Email Finder)**

Para buscar e-mails corporativos:

1. Crie uma conta em https://hunter.io
2. Gere uma API key
3. Adicione ao `.env`:
```env
HUNTER_IO_API_KEY="your-api-key"
```

**Custo:** ~R$ 250/m√™s (50 buscas)

---

### 4. **Receita Federal API (Gr√°tis)**

Busca de dados de CNPJ via BrasilAPI:

- Endpoint: `https://brasilapi.com.br/api/cnpj/v1/{cnpj}`
- **Gr√°tis** e sem necessidade de API key
- J√° implementado e funcional

---

## ü§ñ Como Funciona o Pipeline de Scraping

```
1. LinkedIn Scraping (Bright Data)
   ‚Üì
2. Identificar CNPJ da empresa
   ‚Üì
3. Enriquecer com Receita Federal
   ‚Üì
4. Gerar insights com Claude AI
   ‚Üì
5. Buscar e-mails com Hunter.io (opcional)
   ‚Üì
6. Salvar lead no banco de dados
```

---

## üìä Logs de Scraping

Cada execu√ß√£o do scraping gera um log na tabela `scrape_logs`:

```sql
SELECT * FROM scrape_logs ORDER BY createdAt DESC LIMIT 10;
```

**Campos:**
- `status`: success, error, running
- `query`: Query usada
- `jobsFound`: Total de vagas encontradas
- `leadsCreated`: Total de leads criados
- `duration`: Tempo de execu√ß√£o (segundos)
- `errors`: Erros JSON (se houver)

---

## üöÄ Deploy em Produ√ß√£o

### Vercel

1. Push para GitHub
2. Conecte ao Vercel
3. Configure vari√°veis de ambiente:
   - `DATABASE_URL`
   - `NEXTAUTH_SECRET`
   - `NEXTAUTH_URL`
   - `CRON_SECRET`
   - APIs (Claude, Bright Data, Hunter.io)

4. O cron job ser√° ativado automaticamente via `vercel.json`

### Supabase (Database)

1. Crie projeto em https://supabase.com
2. Copie Connection String
3. Execute: `npx prisma db push`
4. Execute: `npm run db:seed`

---

## üß™ Testando Localmente

### 1. Testar Scraping Manual
```bash
curl -X POST http://localhost:3000/api/scrape \
  -H "Content-Type: application/json" \
  -d '{"query": "Controller S√£o Paulo"}'
```

### 2. Testar Cron Job
```bash
curl http://localhost:3000/api/cron/scrape-leads
```

### 3. Ver Logs
```bash
npx prisma studio
# Abrir tabela "scrape_logs"
```

---

## ‚ö†Ô∏è Limita√ß√µes Atuais

1. **Bright Data**: Implementa√ß√£o base criada, requer configura√ß√£o completa
2. **Hunter.io**: Limite de 50 buscas/m√™s no plano b√°sico
3. **Claude AI**: Custo por token, monitorar uso
4. **CNPJ Search**: Implementa√ß√£o manual pendente

---

## üìö Pr√≥ximos Passos

- [ ] Implementar integra√ß√£o completa com Bright Data
- [ ] Adicionar p√°gina de configura√ß√µes/admin
- [ ] Criar sistema de notifica√ß√µes por e-mail
- [ ] Implementar exporta√ß√£o CSV de leads
- [ ] Adicionar an√°lise de sentimento nos insights
- [ ] Score de prioridade autom√°tico

---

**Documenta√ß√£o atualizada em:** 11/11/2025
